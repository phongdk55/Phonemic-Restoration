import theano
from theano import tensor as T
import numpy as np
from load_data_DNN import load_data, read_features
import math
import os,re
import logging

def floatX(X):
    return np.asarray(X, dtype=theano.config.floatX)

def init_weights(shape):
    return theano.shared(floatX(np.random.randn(*shape) * 0.01))

def sgd(cost, params, lr=0.05):             # generalize to compute gradient descent
    grads = T.grad(cost=cost, wrt=params)   # on all model parameters
    updates = []
    for p, g in zip(params, grads):
        updates.append([p, p - g * lr])
    return updates
'''
def model(X, w_h, w_o):         # 2 layers of computation 
    h = T.nnet.sigmoid(T.dot(X, w_h))       # input -> hidden(signmoid)
    #pyx = T.nnet.softmax(T.dot(h, w_o))     # hidden -> output(softmax)
    pyx = T.nnet.sigmoid(T.dot(h, w_o))     # hidden -> output(softmax)
    return pyx
'''
def model(X, w_h, w_h1, w_h2, w_o):         # 2 layers of computation 
    
    h = T.nnet.sigmoid(T.dot(X, w_h))       # input -> hidden(signmoid)
    h1 = T.nnet.sigmoid(T.dot(h, w_h1))       # hidden -> hidden(signmoid)
    h2 = T.nnet.sigmoid(T.dot(h1, w_h2))       # hidden -> hidden(signmoid)
    #pyx = T.nnet.softmax(T.dot(h, w_o))     # hidden -> output(softmax)
    #pyx = T.nnet.sigmoid(T.dot(h2, w_o))     # hidden -> output(softmax)
    
    #h = T.tanh(T.dot(X, w_h))       # input -> hidden(tanh)
    #h1 = T.tanh(T.dot(h, w_h1))       # hidden -> hidden(tanh)
    #h2 = T.tanh(T.dot(h1, w_h2))       # hidden -> hidden(tanh)
    #pyx = T.nnet.softmax(T.dot(h, w_o))     # hidden -> output(softmax)
    #pyx = T.tanh(T.dot(h2, w_o))     # hidden -> output(softmax)
    pyx = T.dot(h2, w_o)     # hidden -> output
    return pyx

def read_afile(filename, num_features):
    input_file = open(filename,'r')
    feature = np.zeros((num_features))
    arr = []
    for line in input_file:
        sequential_number = line.split(" ")
        #print sequential_number 
        feature = np.zeros((num_features))
        for i in xrange(num_features):
            try:
                feature[i] = (float(sequential_number[i]))
            except:
                continue
        arr.append(feature)
    return np.array(arr).astype(np.float32)

def deep_neural_network():
    trX, trY = read_features()
    print trX.shape
    print trY.shape
    
    X = T.fmatrix()
    Y = T.fmatrix()
    
    nNeural = 64
    w_h = init_weights((109, nNeural))
    w_h1 = init_weights((nNeural, nNeural))
    w_h2 = init_weights((nNeural, nNeural))
    w_o = init_weights((nNeural, 37))
    
    py_x = model(X, w_h, w_h1,w_h2,  w_o)
    y_x = py_x
    
    #cost = T.mean(T.nnet.categorical_crossentropy(py_x, Y))
    cost = T.mean(T.sqr(py_x - Y))
    params = [w_h, w_h1, w_h2, w_o]
    updates = sgd(cost, params)
    train = theano.function(inputs=[X, Y], outputs=cost, updates=updates, allow_input_downcast=True)
    predict = theano.function(inputs=[X], outputs=y_x, allow_input_downcast=True)
    
    LOG_FILENAME = 'DNN.log'
    logging.basicConfig(filename=LOG_FILENAME,level=logging.DEBUG)
    
    for i in range(100):
        #print i
        logging.debug('loop' + str(i))
        for start, end in zip(range(0, len(trX), 128), range(128, len(trX), 128)):
            cost = train(trX[start:end], trY[start:end])
            logging.debug('trX')
            logging.debug(trX[start:end])
            logging.debug('trX')
            logging.debug(trX[start:end])
            logging.debug('w_h')
            logging.debug(w_h.get_value())
            logging.debug('w_h1')
            logging.debug(w_h1.get_value())
            logging.debug('w_h2')
            logging.debug(w_h2.get_value())
            logging.debug('w_o')
            logging.debug(w_o.get_value())
#             print w_h.get_value()
#             print w_h1.get_value()
#             print w_h2.get_value()
#             print w_o.get_value()
            
            #exit()
            if cost is None:
                exit()
            print start , end
            print cost
    #     if i == 199:
    #         print trY[len(trX)-1][0:12]
    #         print predict(trX)[len(trX)-1][0:12]
    #     print np.mean(np.square(trY[:][0:12] - predict(trX)[0:12]))
    
    feature_out_dir = '/home/danglab/Phong/features/output/'
    test_dir = '/home/danglab/Phong/TestData/Features/minus/3dB/'
    dnn_predict_dir = '/home/danglab/DNN_Predict/minus/3dB/'
    
    if not os.path.exists(dnn_predict_dir):
        os.makedirs(dnn_predict_dir)
        
    listtest = sorted(os.listdir(test_dir))
    for afile in listtest:
        #print afile                 #usctimit_ema_f1_001_005_100ms_noise_in.txt
        test_arr = read_afile(test_dir + afile, 109)                                #read a missing_feature
        find_ = [m.start() for m in re.finditer('_', afile)]      
        file_mat = (afile.replace(afile[find_[4]:find_[6]],'')).replace('in.','out.')   #usctimit_ema_f1_001_005_out.txt
        test_res_arr = read_afile(feature_out_dir + file_mat, 13 + 24)              #read a original output feature
        a =  predict(test_arr)
        print a.shape
        print a
        write_predict_2_file(dnn_predict_dir + file_mat.replace("_out",''), predict(test_arr))      # write result to file

        #print test_res_arr[0][0:13]
        #print predict(test_arr)[0][0:13]
        #print np.mean(np.square(test_res_arr[0:13] - predict(test_arr)[0:13]))
        
def write_predict_2_file(filename, res_arr):
    files = open(filename, 'w')
    nframes = res_arr.shape[0]
    for i in xrange(nframes):
        for j in xrange(13):
            files.write("%s " %(res_arr[i][j]) )
        files.write("\n")
    files.close()
    
deep_neural_network()